{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f13651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from dotenv import load_dotenv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba6eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/11 17:26:36 WARN Utils: Your hostname, riddhi, resolves to a loopback address: 127.0.1.1; using 192.168.2.38 instead (on interface wlp47s0)\n",
      "25/07/11 17:26:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/11 17:26:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Update path to match where your JAR file is downloaded - make sure it's absolute path\n",
    "jdbc_jar_path = \"/home/riddhi/Documents/Riddhi-Tech/Projects/Python-Projects/CRUD-Flask/library/postgresql-42.7.3.jar\"\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"PostgreSQLIngestion\") \\\n",
    "        .config(\"spark.jars\", jdbc_jar_path) \\\n",
    "        .config(\"spark.driver.extraClassPath\", jdbc_jar_path) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb47efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PostgreSQLIngestion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b6ed9c36e40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d845b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/riddhi/Documents/Riddhi-Tech/Projects/Python-Projects/CRUD-Flask/library/postgresql-42.7.3.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext._conf.get('spark.jars'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd42bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123abc456 sampledb riddhi_psql users_schema registration\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
    "DB_NAME = os.environ.get('DB_NAME')\n",
    "DB_USER = os.environ.get('DB_USER')\n",
    "SCHEMA_NAME = os.environ.get('SCHEMA_NAME')\n",
    "REGISTRATION_TABLE = os.environ.get('REGISTRATION_TABLE')\n",
    "print(DB_PASSWORD, DB_NAME, DB_USER, SCHEMA_NAME, REGISTRATION_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a33bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://localhost:5432/sampledb {'user': 'riddhi_psql', 'password': '123abc456', 'driver': 'org.postgresql.Driver'} users_schema.registration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This sets up the metadata to ingest your source table.\n",
    "# JDBC URL and properties for PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/\"+ DB_NAME\n",
    "jdbc_properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = f\"{SCHEMA_NAME}.{REGISTRATION_TABLE}\"\n",
    "print(jdbc_url, jdbc_properties, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335196f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- password: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      "\n",
      "+---+------------+--------------------+--------------+--------------------+\n",
      "| id|        name|               email|      password|          created_at|\n",
      "+---+------------+--------------------+--------------+--------------------+\n",
      "|  2|Riddhi Patel|riddhi.patel@exam...|SecurePass123!|2025-06-30 22:26:...|\n",
      "+---+------------+--------------------+--------------+--------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# Read data from PostgreSQL into a DataFrame\n",
    "df = spark.read.jdbc( \\\n",
    "url=jdbc_url, \\\n",
    "table=table_name, \\\n",
    "properties=jdbc_properties \\\n",
    ") \n",
    "\n",
    "# Show the schema and first few rows of the DataFrame\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65eb9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to Parquet format\n",
    "base_dir = \"/home/riddhi/Documents/Riddhi-Tech/Projects/Python-Projects/CRUD-Flask/\"\n",
    "output_dir = os.path.join(base_dir, \"datasets\", \"parquet\", \"registration_data\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "df.write.parquet(f\"{output_dir}/output_data\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c4e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------------+--------------+--------------------+\n",
      "| id|        name|               email|      password|          created_at|\n",
      "+---+------------+--------------------+--------------+--------------------+\n",
      "|  2|Riddhi Patel|riddhi.patel@exam...|SecurePass123!|2025-06-30 22:26:...|\n",
      "+---+------------+--------------------+--------------+--------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(f\"{output_dir}/output_data\")\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c868f11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crud_flask_venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3fbc28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ddb28edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"SCD2_Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "13aaf7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SCD2_Delta</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fabde2b8590>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "01667f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path configurations\n",
    "dim_path =  \"/home/riddhi/Documents/Riddhi-Tech/Projects/Python-Projects/CRUD-Flask/datasets/delta/dim_registration_data/scd_output_data\"\n",
    "raw_path = \"/home/riddhi/Documents/Riddhi-Tech/Projects/Python-Projects/CRUD-Flask/datasets/parquet/registration_data/output_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbca24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read new incoming snapshot\n",
    "incoming_df = spark.read.parquet(raw_path)\n",
    "incoming_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ac334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start_date and default end_date to incoming data\n",
    "DEFAULT_END_DATE = \"2999-12-31 23:59:59\"\n",
    "\n",
    "incoming_df = incoming_df \\\n",
    "    .withColumn(\"start_date\", current_timestamp()) \\\n",
    "    .withColumn(\"end_date\", lit(DEFAULT_END_DATE).cast(TimestampType()))\n",
    "\n",
    "incoming_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if existing SCD-delta data exists\n",
    "if not os.path.exists(dim_path):\n",
    "    # First-time load: no existing SCD-delta data\n",
    "    print(\"SCD-delta table does not exist â€” writing initial snapshot.\")\n",
    "    incoming_df.write.format(\"delta\").mode(\"overwrite\").save(dim_path)\n",
    "else: # add new records to SCD-delta table\n",
    "    # Load existing SCD-delta data\n",
    "    dim_df = spark.read.format(\"delta\").load(dim_path)\n",
    "\n",
    "    # Ensure end_date column is correctly cast\n",
    "    dim_df = dim_df.withColumn(\"end_date\", col(\"end_date\").cast(TimestampType()))\n",
    "\n",
    "    # Filter active records (end_date = default)\n",
    "    active_df = dim_df.filter(col(\"end_date\") == lit(DEFAULT_END_DATE))\n",
    "\n",
    "    #Filter new records (not already present by id)\n",
    "    new_records_df = incoming_df.join(\n",
    "        active_df.select(\"id\"),\n",
    "        on=\"id\",\n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    print(f\"New records found: {new_records_df.count()}\")\n",
    "\n",
    "    #detect updated records\n",
    "    updated_records_df = incoming_df.alias(\"incoming\").join(\n",
    "        active_df.alias(\"active\"),\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    ).filter(\n",
    "        (col(\"incoming.name\") != col(\"active.name\")) |\n",
    "        (col(\"incoming.email\") != col(\"active.email\")) |\n",
    "        (col(\"incoming.password\") != col(\"active.password\"))\n",
    "    ).select(\"incoming.*\")\n",
    "\n",
    "   # If updated records are found, set their end_date to current timestamp, as we need to add them as new versions\n",
    "    expired_updated_df = active_df.join(\n",
    "        updated_records_df.select(\"id\"),\n",
    "        on=\"id\",\n",
    "        how=\"inner\"\n",
    "    ).withColumn(\"end_date\", current_timestamp())\n",
    "\n",
    "    # Detect deleted records\n",
    "    deleted_records_df = active_df.join(\n",
    "        incoming_df.select(\"id\"),\n",
    "        on=\"id\",\n",
    "        how=\"left_anti\"\n",
    "    ).withColumn(\"end_date\", current_timestamp())\n",
    "\n",
    "    print(f\"Deleted records found: {deleted_records_df.count()}\")\n",
    "    \n",
    "    # Union deleted records with expired updated records (records to be marked as inactive)\n",
    "    inactive_records_df = deleted_records_df.unionByName(expired_updated_df)\n",
    "\n",
    "    # Remove old active versions for deleted records from active_df\n",
    "    remaining_active_df = active_df.join(\n",
    "    inactive_records_df.select(\"id\"),\n",
    "    on=\"id\",\n",
    "    how=\"left_anti\"\n",
    "    )\n",
    "\n",
    "    #Union and write back:\n",
    "    # - all non-active history\n",
    "    # - remaining active (not deleted)\n",
    "    # - new records\n",
    "    # - deleted records (with updated end_date)\n",
    "    # - updated records\n",
    "    final_df = dim_df.filter(col(\"end_date\") != lit(DEFAULT_END_DATE)) \\\n",
    "                .unionByName(new_records_df) \\\n",
    "                .unionByName(deleted_records_df) \\\n",
    "                .unionByName(remaining_active_df) \\\n",
    "                .unionByName(updated_records_df.withColumn(\"end_date\", lit(DEFAULT_END_DATE).cast(TimestampType())))\n",
    "\n",
    "    final_df.show(10, truncate=False)\n",
    "    final_df.write.format(\"delta\").mode(\"overwrite\").save(dim_path) \n",
    "    print(\"Updated SCD-delta table written successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36872618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                           |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-07-14 17:28:11.45 |NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numFiles -> 6, numRemovedFiles -> 1, numRemovedBytes -> 2857, numOutputRows -> 7, numOutputBytes -> 15019}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|0      |2025-07-14 17:24:24.902|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 5, numOutputBytes -> 2857}    |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------+--------------------------------------+----+--------+---------+-----------+--------------+-------------+-----------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Delta table history\n",
    "delta_table = DeltaTable.forPath(spark, dim_path)\n",
    "history_df = delta_table.history()  # By default, returns last 20 operations\n",
    "history_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Delta table at version 0\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(dim_path)\n",
    "df_v0.show(50, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crud_flask_venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
